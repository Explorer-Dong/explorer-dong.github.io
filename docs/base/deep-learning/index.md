---
title: 深度学习导读
---

## 前言

本文记录深度学习入门笔记。教材主要参考邱锡鹏老师的神经网络与深度学习 [^book] [^nndl] [^nndl github]，框架以 [PyTorch](https://pytorch.org/docs/stable/index.html) 为主，[TensorFlow](https://tensorflow.google.cn/api_docs/python/tf) 为辅。比较推荐 [Karpathy 的教学视频](https://space.bilibili.com/3129054/lists/874339) 用来视频自学，[《动手学深度学习（PyTorch 版）》](https://zh.d2l.ai/index.html) 用来动手实践。

[^book]: 邱锡鹏.《神经网络与深度学习》[M]. 1版. 北京: 机械工业出版社, 2017. ISBN 978-7-111-64968-7.
[^nndl]: <https://nndl.github.io/>
[^nndl github]: <https://github.com/nndl/>
## 绪论

**表示学习是什么**？与传统的特征工程目的一致，为了得到数据中的更好的特征。不同的是，特征工程中的特征选择、特征映射等策略都是可控的方式，而表示学习就是利用深度学习从数据中学习高层的有效特征。

**深度学习是什么**？我们知道机器学习就是在手动处理完特征后，构建对应的模型 **预测输出**。而深度学习就是将机器学习的手动特征工程也用模型进行 **表示学习** 来学习出有效特征，然后继续构建模型 **预测输出**。如下图所示：

![深度学习的数据处理流程](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202409121508150.png)

**为什么会有深度学习**？最简单的一点就是，很多特征我们根本没法定义一种表示规则来表示特征，比如说对于图像，怎么定义复杂的图像的特征呢？比如说对于音频，又怎么定义复杂的音频的特征呢？没办法，我们直接学特征！

**神经网络是什么**？就是万千模型中的一种，仅此而已。

**为什么用神经网络进行深度学习**？有了上面对深度学习定义的理解，可以发现其中最具有挑战性的特点就是，模型怎么知道什么才是好特征？什么是不好的特征？神经网络可以很好的解决这个问题。通过由浅到深层层神经元的特征提取，深层的神经元就可以学习更高语义的特征，并且通过这种方式也可以实现「**端到端**」的学习方式，而不用像机器学习的方法那样，先选特征再学习。并且神经网络模型也可以很好的解决深度学习中特征的「**贡献度分配**」问题。

在机器学习中，我们学习到了诸如线性模型、决策树模型、贝叶斯模型等模型，下面我们将正式开始深度学习中神经网络模型的学习。

## 感知器

### 神经元模型

![M-P 神经元模型](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404021447476.png)

我们介绍 M-P 神经元模型。该神经元模型必须具备以下三个特征：

1. 输入：来自其他连接的神经元传递过来的输入信号
2. 处理：输入信号通过带权重的连接进行传递，神经元接受到所有输入值的总和，再与神经元的阈值进行比较
3. 输出：通过激活函数的处理以得到输出

激活函数可以参考 3.3 中的逻辑函数（logistic function），此处将其声明为 sigmoid 函数，同样不采用不。连续光滑的分段函数。

### 感知机与多层网络

本目从 **无隐藏层的感知机** 出发，介绍神经网络在简单的线性可分问题上的应用；接着介绍 **含有一层隐藏层的多层感知机**，及其对于简单的非线性可分问题上的应用；最后引入多层前馈神经网络模型的概念。

感知机。

![感知机（Perceptron）](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404021609339.png)

感知机（Perceptron）由两层神经元组成。第一层是输入层，第二层是输出层。其中只有输出层的神经元为功能神经元，也即 M-P 神经元。先不谈如何训练得到上面的 $w_1,w_2,\theta$，我们先看看上面的感知机训练出来以后可以有什么功能？

通过单层的感知机，我们可以实现简单的线性可分的分类任务，比如逻辑运算中的 **与、或、非** 运算，下面演示一下如何使用单层感知机实现上述三种逻辑运算：

??? note "使用单层感知机实现线性可分任务：与、或、非三种逻辑运算"

    与运算、或运算是二维线性可分任务，一定可以找到一条直线将其划分为两个类别：
    
    ![二维线性可分任务](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404091955554.png)
    
    非运算是一维线性可分任务，同样也可以找到一条直线将其划分为两个类别：
    
    ![一维线性可分任务](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404091959633.png)

多层感知机。

![神经网络图例：多层感知机](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404021604059.png)

所谓的多层感知机其实就是增加了一个隐藏层，则神经网络模型就变为三层，含有一个输入层，一个隐藏层，和一个输出层，更准确的说应该是“单隐层网络”。其中隐藏层和输出层中的所有神经元均为功能神经元。

为了学习出网络中的连接权 $w_i$ 以及所有功能神经元中的阈值 $\theta_j$，我们需要通过每一次迭代的结果进行参数的修正，对于连接权 $w_i$ 而言，我们假设当前感知机的输出为 $\hat y$，则连接权 $w_i$ 应做以下调整。其中 $\eta$ 为学习率。

$$
\begin{aligned}
w_i \leftarrow w_i + \Delta w_i \\
\Delta w_i = \eta (y - \hat y) x_i
\end{aligned}
$$

??? note "使用多层感知机实现异或逻辑运算"

    ![使用多层感知机实现异或逻辑运算](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404092000730.png)

多层前馈神经网络。

![多层前馈神经网络结构示意图](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404021604208.png)

所谓多层前馈神经网络，定义就是各层神经元之间不会跨层连接，也不存在同层连接，其中：

- 输入层仅仅接受外界输入，没有函数处理功能
- 隐藏层和输出层进行函数处理

### 误差逆传播算法

多层网络的学习能力比感知机的学习能力强很多。想要训练一个多层网络模型，仅仅通过感知机的参数学习规则是不够的，我们需要一个全新的、更强大的学习规则。这其中最优秀的就是误差逆传播算法 (errorBackPropagation, BP)，往往用它来训练多层前馈神经网络。下面我们来了解一下 BP 算法的内容、参数推导与算法流程。

模型参数。

我们对着神经网络图，从输入到输出进行介绍与理解：

![单隐层神经网络](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404090923723.png)

- 隐层：对于隐层的第 $h$ 个神经元
    - 输入：$\alpha_h = \sum_{i=1}^dx_i v_{ih}$
    - 输出：$b_h = f(\alpha_h - \gamma_h)$
- 输出层：对于输出层的第 $j$ 个神经元
    - 输入：$\beta_j=\sum_{h=1}^q b_h w_{hj}$
    - 输出：$\hat y_j = f(\beta j - \theta_j)$

现在给定一个训练集学习一个分类器。其中每一个样本都含有 $d$ 个特征，$l$ 个输出。现在使用 **标准 BP 神经网络模型**，每输入一个样本都迭代一次。对于单隐层神经网络而言，一共有 4 种参数，即：

- 输入层到隐层的 $d \times q$ 个权值 $v_{ih}(i=1,2,\cdots,d,\ h=1,2,\cdots,q)$
- 隐层的 $q$ 个 M-P 神经元的阈值 $\gamma_h(h=1,2,\cdots,q)$
- 隐层到输出层的 $q\times l$ 个权值 $w_{hj}(h=1,2,\cdots,q,\ j=1,2,\cdots,l)$
- 输出层的 $l$ 个 M-P 神经元的阈值 $\theta_j(j=1,2,\cdots,l)$

参数推导。

确定损失函数。

- 对于上述 4 种参数，我们均采用梯度下降策略。**以损失函数的负梯度方向对参数进行调整**。每次输入一个训练样本，都会进行一次参数迭代更新，这叫 **标准 BP 算法**。

- 根本目标是使损失函数尽可能小，我们定义损失函数 $E$ 为当前样本的均方误差，并为了求导计算方便添加一个常量 $\frac{1}{2}$，对于第 $k$ 个训练样本，有如下损失函数：

$$
E_k = \frac{1}{2} \sum _{j = 1}^l (\hat y_j^k - y_j^k)^2
$$

确定迭代修正量。

- 假定当前学习率为 $\eta$，对于上述 4 种参数的迭代公式为：

    $$
    \begin{aligned}
    w_{hj} &\leftarrow w_{hj}+\Delta w_{hj} \\
    \theta_{j} &\leftarrow \theta_{j}+\Delta \theta_{j} \\
    v_{ih} &\leftarrow v_{ih}+\Delta v_{ih} \\
    \gamma_{h} &\leftarrow \gamma_{h}+\Delta \gamma_{h} \\
    \end{aligned}
    $$
    
- 其中，修正量分别为：

    $$
    \begin{aligned}
    \Delta w_{hj} &= \eta g_j b_h \\
    \Delta \theta_{j} &= -\eta g_j \\
    \Delta v_{ih} &= \eta e_h x_i \\
    \Delta \gamma_{h} &= -\eta e_h \\
    \end{aligned}
    $$

??? note "修正量推导 - 链式法则"

    公式表示：
    
    ![公式表示](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404092222942.jpg)
    
    隐层到输出层的权重、输出神经元的阈值：
    
    ![隐层到输出层的权重、输出神经元的阈值](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404092222625.jpg)
    
    输入层到隐层的权重、隐层神经元的阈值：
    
    ![输入层到隐层的权重、隐层神经元的阈值](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404092223804.jpg)

算法流程。

对于当前样本的输出损失 $E_k$ 和学习率 $\eta$，我们进行以下迭代过程：

![BP 神经网络算法流程](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404090920527.png)

还有一种 BP 神经网络方法就是 **累计 BP 神经网络** 算法，基本思路就是对于全局训练样本计算累计误差，从而更新参数。在实际应用过程中，一般先采用累计 BP 算法，再采用标准 BP 算法。还有一种思路就是使用随机 BP 算法，即每次随机选择一个训练样本进行参数更新。

## 前馈神经网络

前馈神经网络 (Feedforward Neural Network) 就是网络模型中的全连接层。通过向前传递数据，向后更新参数，实现学习和拟合的功能。

前馈神经网络模型的学习准则一般采用交叉熵损失，优化算法一般采用小批量随机梯度下降 (Mini-batch Stochastic Gradient Descent, 简称 Mini-batch SGD) 算法。

### 神经元

**网络模型中的最小学习单元是什么**？神经元。每一个神经元接受输入 $z$，通过设定好的激活函数 $f$，给出输出 $a=f(z)$。神经元中的激活函数大致种类极多，主要有以下 3 种：

- S 型函数。Sigmoid 函数。例如 logistic 函数和 Tanh 函数。
- 斜坡函数。Relu 函数。
- Swish 函数。复合函数。

**激活函数的 4 个原则是什么？为什么**？。非线性、可导、单调、有界。为了确保网络可以拟合复杂的映射关系，需要激活函数是非线性的；为了便于对网络求导从而进行参数更新，需要确保激活函数是可导的；为了防止对网络求导的过程中出现梯度消失或者梯度爆炸，需要确保激活函数是单调并且有界。

### 反向传播算法

注：我们定义输入层为前，输出层为后。

在进行随机梯度下降时。经过简单的推导可以发现，第 $l$ 层的损失 $\delta (l)$ 依赖于后一项的损失 $\delta(l+1)$，于是每一个样本更新参数的逻辑就是从输出层开始逐层往前直到第一层隐藏层进行更新。

## 卷积神经网络

为了解决全连接网络不能学习到局部不变性的缺点，卷积网络应运而生。卷积网络有三大特征：局部连接、权重共享、时/间上的次采样。

### 卷积方式

等宽卷积、宽卷积、窄卷积；

转置卷积；

空洞卷积。

### 卷积网络

卷积神经网络的典型结构如下图所示：

![卷积神经网络的典型结构](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202410311351161.png)

/// fc
卷积神经网络的典型结构
///

**卷积层**。对于二维卷积而言，我们假设输入共有 D 个图像，需要学习 P 个特征，并不是只需要学习 P 个卷积核的，对于每一个特征需要单独对每一张图像学习一个卷积核，因此需要学习 $D\times P$ 个卷积核。

**汇聚层 / 池化层**。从上面的卷积层学习逻辑可以看出其实参数量和全连接层相比还是很大。池化层就是通过压缩卷积核大小来降低参数量；与此同时，通过池化也可以起到去噪作用。

其他卷积网络：

- AlexNet [2012]。使用了 ReLU 激活函数，使用 GPU 并行计算，使用了数据增强；
- VGGNet。
- Inception 网络 [v3, 2016]。将多个等宽卷积的结果进行堆叠；
- 残差网络。由于恒等模型 $f(x)=x$ 无法学习到，故将线性模型转化为 $f(x) = x + [f(x) - x]$ 并来学习新的模型 $f(x)-x$。可以解决梯度消失的问题；
- GoogleNet。
- DenseNet。

### 参数学习

同样是反向传播算法通过梯度下降进行优化。

## 循环神经网络

监督学习任务：

1. 序列到类别模式。例如「序列分类」中的情感分类任务：给定一个语言序列，分析序列的类别；
2. 同步的序列到序列。例如「序列标注」任务中的中文分词：给定一个中文序列，对序列中每一个词语进行词性分析；
3. 异步的序列到序列。例如「序列编码解码」任务中的机器翻译：输入序列和输出序列不需要有长度的严格对应关系。

时序预测任务：时序序列的数据集形如 $\{ t_i,\boldsymbol{x}_i, y_i \}$

- 传统机器学习方法：自回归模型。具体的，模型 $y_i=\beta_0 + \beta_1 y_{i-1} + \beta_2 y_{i-2} + \cdots + \beta_p y_{i-p}$ 被称为 P 阶自回归模型。可以看出这种模型仅仅利用到了标签值并且定义当前标签值与曾经的标签值呈线性关系；
- 前沿深度学习方法：RNN 模型、LSTM 模型、时序卷积模型、Transformer。

## 网络优化

为了能够让一个网络模型「训练速度更快」和「泛化性能更好」，我们可以采用各种网络优化策略。针对一个神经网络，可以有如下五个角度的网路优化策略。

更好的优化算法：

![更好的优化算法](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202412201527036.png)

更好的参数初始化方法：基于「范数保持性」的参数初始化方法。

更好的数据预处理方法：逐层归一化。

更好的网络结构：ReLU 激活函数、残差连接。

更好的超参数优化方法：

超参数：

- 层数
- 每层神经元个数
- 激活函数
- 学习率（以及动态调整算法）
- 正则化系数
- mini-batch 大小

超参数优化方法：

- 网格搜索
- 随机搜索
- 贝叶斯优化
- 动态资源分配
- 神经架构搜索

**mini-batch 与学习率的关系**。对于网络中的超参数，如何进行选择呢？最朴素的方法就是网格搜索。对于随机梯度下降的参数优化算法，批大小与学习率一般成正比，即一批训练数据量越多，学习率越高。这是因为一批的训练数据越多，泛化能力就越高，对应的学习率就没必要太低。

**学习率的动态调整算法**。一般来说就是两个阶段，在初期的阶段，学习率线性增长，在之后的阶段中，学习率逐渐衰减。详情见 Facebook 的这篇论文 [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour [2018]](https://arxiv.org/pdf/1706.02677)。

下图展示了预热学习率调整的学习效果。图源：[Bag of Tricks for Image Classification with Convolutional Neural Networks [2018]](https://arxiv.org/abs/1812.01187v2)。

![Visualization of learning rate schedules with warm-up.  Top:  cosine and step schedules for batch size 1024. Bottom: Top-1 validation accuracy curve with regard to the two schedules.](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202412201601516.jpg)

## 注意力机制与外部记忆

## 序列生成模型

## 深度生成模型

本章我们学习深度生成模型。用一句话概括就是使用深度神经网络训练一个模型使得其输出结果符合某个预定的分布。

- 变分自编码器 VAE

- 生成对抗网络 GAN

- Stable Diffusion：经典的文生图模型。分为以下三个部分，这三个部分均可以单独训练：

    - 编码器

    - 生成模型

    - 解码器
