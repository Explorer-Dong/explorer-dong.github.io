---
title: 数据处理
---

!!! quote
    数据决定上限，模型和算法只是尽可能逼近这个上限。

本文记录「关系型数据」的基本处理方法。不同的数据有不同的处理方法：

1. 结构化数据。比如字段明确的关系型数据和结构明确的知识图谱数据；
2. 半结构化数据。常见的有 HTML、XML、JSON 等，这一类数据有比较好的结构特点，但是内容依然很复杂，无法直接处理。现在有很多成熟的解析器可以将半结构化数据转换为对应标签下的非结构化数据，因此本质上还是在处理非结构化数据；
3. 非结构化数据。常见的有文本、图像、视频、音频等。不同的数据对应到不同的研究领域。比如文本数据对应到 [自然语言处理](../natural-language-processing/index.md) 领域，图像 / 视频数据对应到 [计算机视觉](../computer-vision/index.md) 领域，音频数据对应到 [语音信号处理](../speech-signal-processing/index.md) 领域。在对应的笔记中都有详细的数据预处理方法，这里不再赘述。

接下来主要介绍数据预处理的几个主要任务，将「关系型数据」处理成模型可接受的形式并且提升数据的质量。

## 数据分析

在拿到一个数据集时，可以先简单分析一下各个变量的统计特点，再做进一步的数据处理。

一些常见的统计量如下：

- 中心趋势度量。用来描述数据集中心位置的统计量，它反映了数据的平均水平或典型值。例如：
    - 算术平均数（受计算数据的影响大）；
    - 调和平均数（特定场景）；
    - 中位数（适用于序数申诉信，表示位置信息，不受极差影响）；
    - 众数（不受极差影响）。
- 离散度度量。用来描述数据分布的广泛程度，即数据值偏离其中心趋势的程度。例如：
    - 极差（适用于数据极端值较少且分布不复杂的场景）；
    - 标准差（解释性比方差更好，反应数据与均值之间的关系，对极端值敏感）；
    - 分位数（反应数据内部的离散程度，容易忽略极端数据）。

有了上述统计量，可以结合可视化方法直观地呈现出来，例如：

- 箱型图、五数概括、直方图。有助于可视化「单个属性」的数据分布；
- 饼图。有助于可视化「单个属性」的数据占比；
- 散点图。有助于可视化「两个属性」的相关关系。

## 数据清洗

**缺失值处理**。可以直接删除含有缺失属性的样本，也可以对含有缺失属性的样本进行填充。填充的策略比较多：

1. 简单点可以用平均数、众数、窗口均值等统计值填充；
2. 稍复杂可以用前推法、后推法、插值法等策略进行填充；
3. 甚至还可以用贝叶斯估计等概率估计策略进行填充。

**噪声处理**。可以用箱型图去掉噪声，也可以对数据做平滑处理，常见的平滑策略如下：

- 函数拟合平滑。比较显然，就是利用线性回归把数据线性映射一下；
- 近邻局部平滑。也称分箱法，我们将原始数据排序后，通过「等深分箱（按元素数量）」或「等宽分箱（按元素值域）」或「自定义分箱」等策略划分为不同的子集，然后对每一个子集使用其统计值进行平滑处理；
- 指数平滑。也称递推法，人工定义一个权重参数 $\alpha \in (0,1)$，将原始数据记作 $x_1,x_2,...,x_n$，平滑后数据记作 $s_1,s_2,...,s_n$，则有 $s_i = \alpha x_i + (1 - \alpha) s_{i-1}$。


## 数据离散

数据离散化就是对连续属性值的属性进行离散化操作，从而适用于只能处理离散属性的场景。

## 数据规范

数据规范就是所谓的「归一化」，避免了属性之间不同量纲的影响。常用的有：

- 最大最小规范化。规范公式为 $\displaystyle x'= \frac{x - x_{\min}}{x_{\max} - x_{\min}}(r - l) + l$。缺点是容易受离群点的影响，且测试数据可能会超过阈值 $[l,r]$；
- 均值标准差规范化。规范公式为 $\displaystyle x'=\frac{x - \overline{x}}{\sigma_x}$。缺点是计算量相对更大；
- 零均值规范化。规范公式为 $\displaystyle x'=\frac{x}{10^j}$，其中 $j$ 为使得 $\displaystyle \frac{\max{|x|}}{10^j}<1$ 的最小取值。

## 特征工程

经过上述步骤，数据处理的第一阶段就算完成了。但此时数据特征的维度往往很大，我们希望在「保持甚至提升模型性能」的情况下降低特征的维度，这样也有助于提升模型的训练速度。

常见的特征筛选方法主要有两个策略。一种是不改变特征数值的「选择」策略，即通过某种规则筛选出特定的特征；另一种是改变特征数值的「映射」策略，即通过某种算法将数据映射到更低的维度。

### 特征选择

*A Comprehensive Survey on the Process, Methods, Evaluation, and Challenges of Feature Selection* [^feat-select-survey] 完整的讲解了特征选择的方法，下面两篇文章对其进行了解读：

- [特征选择方法汇总 | 孙佳伟 - (zhuanlan.zhihu.com)](https://zhuanlan.zhihu.com/p/74198735)
- [机器学习之特征选择 | s1awwhy - (www.cnblogs.com)](https://www.cnblogs.com/s1awwhy/p/14067489.html)

[^feat-select-survey]: M. R. Islam, A. A. Lima, S. C. Das, M. F. Mridha, A. R. Prodeep and Y. Watanobe, "A Comprehensive Survey on the Process, Methods, Evaluation, and Challenges of Feature Selection," in *IEEE Access*, vol. 10, pp. 99595-99632, 2022, doi: 10.1109/ACCESS.2022.3205618.

### 特征映射

特征映射（也就是所谓的降维）同样是减少数据的特征维度，相比于直接删除某些特征，特征映射会在压缩特征维度的基础上尽可能保留最佳特征。下面介绍几种降维算法。

1）**多维缩放 (multidimensional scaling, MDS)**

一种无监督降维算法，旨在保持样本间距离（如欧氏距离）不变的情况下将高维数据映射到低维空间。该算法的原则：对于任意的两个样本，降维后两个样本之间的距离保持不变。

我们定义 $b_{ij}$ 为降维后任意两个样本之间的内积，$dist_{ij}$ 表示任意两个样本的原始距离，$Z \in R^{d'\times m},d' \le d$ 为降维后数据集的特征矩阵。可以得到以下降维流程：

内积计算：

$$
b_{ij}=-\frac{1}{2}(dist_{ij}^{2}-dist_{i\cdot}^{2}-dist_{\cdot j}^{2}+dist_{\cdot\cdot}^{2})
$$

新属性值计算：特征值分解法。其中 $B = V \Lambda V^T$

$$
\mathbf{Z}=\mathbf{\Lambda}_*^{1/2}\mathbf{V}_ *^\mathrm{T}\in\mathbb{R}^{d^*\times m}
$$

2）**主成分分析 (Principal Component Analysis, PCA)**

一种无监督降维算法，通过线性变换将数据投影到方差最大的正交方向（主成分），实现降维并保留主要信息。该算法有两个原则：样本到超平面的距离都尽可能近、样本在超平面的投影都尽可能分开。基于此思想可以得到 PCA 算法流程：

![PCA 算法流程](https://cdn.dwj601.cn/images/202406040911195.png)

3）**线性判别分析 (Linear Discriminant Analysis, LDA)**

一种监督降维方法，通过最大化类间散度与最小化类内散度的比值，将数据投影到低维空间。它假设各类数据服从相同协方差的高斯分布。

## 数据划分

特征处理完以后，我们就可以对数据集进行划分，以用于后续的训练、验证和测试任务。

### 留出法

即 Hold Out。将数据集分为「训练集、验证集和测试集」三个部分。测试集对于训练是完全未知的，我们划分出测试集是为了模拟未来未知的数据。当下的任务就是利用训练集训练出最佳模型来尽可能好的拟合验证集，进而在测试集上得出模型的最终指标。

### 自助法

即 Bootstrapping。在数据集 $D$ 中进行「有放回」采样得到训练集 $D'$，将剩余的 $D-D'$ 作为测试集。

- 结论：从 $D\ (|D|=m)$ 中有放回采样 $m$ 次得到的样本集合 $D'$ 大约占 $D$ 的 $\frac{2}{3}$；
- 证明：我们计算测试集比例，由于一次采样中某个样本没有被选中的概率为 $1-\frac{1}{m}$，那么 $m$ 次采样中某个样本从没被选中的概率就是 $(1-\frac{1}{m})^m$，而 $\lim_{m\to+\infty}(1-\dfrac{1}{m})^m = \frac{1}{e}\approx 0.368 \approx \frac{1}{3}$，那么就可以得出训练集 $D'$ 大约就占总数据集的 $\frac{2}{3}$，得证。

### 交叉验证法

即 Cross Validation。一般为 $k$ 折交叉验证。即将数据集随机划分为 $k$ 个大小相似的互斥子集，将其中 $k-1$ 份作为训练集，$1$ 份作为测试集。算法执行 $k$ 次获得平均误差。该方法避免了因为数据划分的随机性带来的系统误差。

## 数据平衡

在分类任务的数据集中，往往会出现类别不平衡的问题。即使在类别平衡时，使用一对其余等策略进行多分类时也会出现类比不平衡的问题，因此解决类比不平衡问题十分关键。

!!! warning
    数据平衡操作只能用于 **训练集**，验证集和测试集不能进行任何分布上的改变。

### 阈值移动

对于二分类任务。我们定义 $y$ 为样本预测为正例的概率，当训练集中正负样例比例相近时，一般以 $0.5$ 为划分依据，即 $y>0.5$ 的判定为正例，反之为负例。简单修改一下 $y>0.5$ 就得到了下式：

$$
\frac{y}{1-y}> 1
$$

但当数据不平衡时，上式就不合理了。我们令 $m^+$ 为训练集中样本正例的数量，$m^-$ 为训练集中样本负例的数量，此时应当以 $\dfrac{m^+}{m^+ + m^-}$ 为划分依据，即 $y>\dfrac{m^+}{m^+ + m^-}$ 的判定为正例，反之为负例。简单修改一下 $y>\dfrac{m^+}{m^+ + m^-}$ 就得到了下式：

$$
\frac{y}{1-y} > \frac{m^+}{m^-}
$$

值得注意的是，由于训练集往往不能遵循独立同分布原则，也就导致我们观测的 $\dfrac{m^+}{m^-}$ 其实不能准确代表真实的正负比例。

### 欠采样

去除过多的样本从而平衡正负例数量。

- 优点：训练的时间开销小；
- 缺点：可能会丢失重要信息。

典型算法：EasyEnsemble。

### 过采样

添加额外的样本从而平衡正负例数量。

- 优点：模型的拟合能力可以得到进一步提升；
- 缺点：简单的重复采样会导致模型过拟合数据，缺少泛化能力、模型训练开销变大。

典型算法：SMOTE。
